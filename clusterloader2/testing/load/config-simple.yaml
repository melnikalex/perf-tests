# ASSUMPTIONS:
# - Underlying cluster should have 100+ nodes.
# - Number of nodes should be divisible by NODES_PER_NAMESPACE (default 100).
# - The number of created SVCs is half the number of created Deployments.
# - Only half of Deployments will be assigned 1-1 to existing SVCs.

# Overrides
{{$CL2_USE_HOST_NETWORK_PODS := false}}
{{$SMALL_STATEFUL_SETS_PER_NAMESPACE := 0}}
{{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE := 0}}
{{$CL2_ENABLE_PVS := false}}
{{$PROMETHEUS_SCRAPE_KUBE_PROXY := false}}
{{$ENABLE_SYSTEM_POD_METRICS := false}}
{{$NODE_MODE := "master"}}
{{$Nodes := 1000}}

#Constants
# Cater for the case where the number of nodes is less than nodes per namespace. See https://github.com/kubernetes/perf-tests/issues/887
{{$NODES_PER_NAMESPACE := 100}}
# See https://github.com/kubernetes/perf-tests/pull/1667#issuecomment-769642266
# https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt
{{$PODS_PER_NODE := 8}}
{{$LOAD_TEST_THROUGHPUT := DefaultParam .CL2_LOAD_TEST_THROUGHPUT 10}}
{{$DELETE_TEST_THROUGHPUT := DefaultParam .CL2_DELETE_TEST_THROUGHPUT $LOAD_TEST_THROUGHPUT}}
{{$BIG_GROUP_SIZE := DefaultParam .BIG_GROUP_SIZE 250}}
{{$MEDIUM_GROUP_SIZE := DefaultParam .MEDIUM_GROUP_SIZE 30}}
{{$SMALL_GROUP_SIZE := DefaultParam .SMALL_GROUP_SIZE 5}}
{{$SMALL_STATEFUL_SETS_PER_NAMESPACE := 1}}
{{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE := 1}}
{{$ENABLE_API_AVAILABILITY_MEASUREMENT := DefaultParam .CL2_ENABLE_API_AVAILABILITY_MEASUREMENT false}}
{{$ENABLE_HUGE_SERVICES := DefaultParam .CL2_ENABLE_HUGE_SERVICES false}}
# Determines number of pods per deployment. Should be a divider of .Nodes.
{{$HUGE_SERVICES_SIZE := DefaultParam .CL2_HUGE_SERVICES_SIZE 1000}}
{{$RANDOM_SCALE_FACTOR := 0.5}}
#Variables
{{$namespaces := DivideInt .Nodes $NODES_PER_NAMESPACE}}
{{$totalPods := MultiplyInt $namespaces $NODES_PER_NAMESPACE $PODS_PER_NODE}}
{{$podsPerNamespace := DivideInt $totalPods $namespaces}}
{{$saturationTime := DivideInt $totalPods $LOAD_TEST_THROUGHPUT}}
{{$deletionTime := DivideInt $totalPods $DELETE_TEST_THROUGHPUT}}
# bigDeployments - 1/4 of namespace pods should be in big Deployments.
{{$bigDeploymentsPerNamespace := 2}}
# mediumDeployments - 1/4 of namespace pods should be in medium Deployments.
{{$mediumDeploymentsPerNamespace := DivideInt $podsPerNamespace (MultiplyInt 4 $MEDIUM_GROUP_SIZE)}}
# smallDeployments - 1/2 of namespace pods should be in small Deployments.
{{$smallDeploymentsPerNamespace := DivideInt $podsPerNamespace (MultiplyInt 2 $SMALL_GROUP_SIZE)}}

# The minimal number of pods to be used to measure various things like
# pod-startup-latency or scheduler-throughput. The purpose of it is to avoid
# problems in small clusters where we wouldn't have enough samples (pods) to
# measure things accurately.
{{$MIN_PODS_IN_SMALL_CLUSTERS := 500}}

name: load
namespace:
  number: {{$namespaces}}
  prefix: test-ECS_TASK_ID

tuningSets:
- name: Sequence
  parallelismLimitedLoad:
    parallelismLimit: 1
# TODO(https://github.com/kubernetes/perf-tests/issues/1024): This TuningSet is used only for pod-startup-latency, get rid of it
# Uniform5qps: for each running phase, use 5 qps.
- name: Uniform5qps
  qpsLoad:
    qps: 5
# Global100qps: use 100 qps globally:
# * split equally qps among all running phases
# * if some phase finishes, other phases will get more qps.
- name: Global100qps
  globalQPSLoad:
    qps: 100
    burst: 1
- name: RandomizedSaturationTimeLimited
  RandomizedTimeLimitedLoad:
    timeLimit: 300s
- name: RandomizedScalingTimeLimited
  RandomizedTimeLimitedLoad:
    # The expected number of created/deleted pods is totalPods/4 when scaling,
    # as each RS changes its size from X to a uniform random value in [X/2, 3X/2].
    # To match 10 [pods/s] requirement, we need to divide saturationTime by 4.
    timeLimit: {{DivideInt $saturationTime 4}}s
- name: DeletionTimeLimited
  TimeLimitedLoad:
    timeLimit: 5m
- name: RandomizedDeletionTimeLimited
  RandomizedTimeLimitedLoad:
    timeLimit: {{$deletionTime}}s
- name: forever-qps
  timedQPSLoad:
    qps: 2
    duration: 1000000h

steps:
- name: list-pods-load
  measurements:
  # I hacked WaitForRunningPods to be kick a go routine async,
  # so this step starts 10 go routines that do LIST queries at the provided qps
    - Method: WaitForRunningPods
      Instances:
      - Identifier: list-pods-1
      - Identifier: list-pods-2
      - Identifier: list-pods-3
      - Identifier: list-pods-4
      - Identifier: list-pods-5
      - Identifier: list-pods-6
      - Identifier: list-pods-7
      - Identifier: list-pods-8
      - Identifier: list-pods-9
      - Identifier: list-pods-10
      - Identifier: list-pods-11
      - Identifier: list-pods-12
      - Identifier: list-pods-13
      - Identifier: list-pods-14
      - Identifier: list-pods-15
      - Identifier: list-pods-16
      - Identifier: list-pods-17
      - Identifier: list-pods-18
      - Identifier: list-pods-19
      - Identifier: list-pods-20
      Params:
        timeout: 1000000h
        desiredPodCount: 999999999 # 3 namespaces, 3 deployments per namespace, 3 pods per deployment
        qps: 4

- module:
    path: modules/services.yaml
    params:
      actionName: "Creating"
      namespaces: {{$namespaces}}
      smallServicesPerNamespace: {{DivideInt (AddInt $smallDeploymentsPerNamespace 1) 2}}
      mediumServicesPerNamespace: {{DivideInt (AddInt $mediumDeploymentsPerNamespace 1) 2}}
      bigServicesPerNamespace: {{DivideInt (AddInt $bigDeploymentsPerNamespace 1) 2}}

- name: Creating PriorityClass for DaemonSets
  phases:
  - replicasPerNamespace: 1
    tuningSet: Sequence
    objectBundle:
      - basename: daemonset-priorityclass
        objectTemplatePath: daemonset-priorityclass.yaml

# Moved from reconcile-objects.yaml to mitigate https://github.com/kubernetes/kubernetes/issues/96635.
# TODO(https://github.com/kubernetes/perf-tests/issues/1823): Merge back to reconcile-objects.yaml once the k/k bug is fixed.
- module:
    path: /modules/configmaps-secrets.yaml
    params:
      actionName: create
      tuningSet: Global100qps
      namespaces: {{$namespaces}}
      bigDeploymentsPerNamespace: {{$bigDeploymentsPerNamespace}}
      mediumDeploymentsPerNamespace: {{$mediumDeploymentsPerNamespace}}
      smallDeploymentsPerNamespace: {{$smallDeploymentsPerNamespace}}

- module:
    path: /modules/reconcile-objects.yaml
    params:
      # we have to create before loop update forever cause the verb is determined at compile time
      # so if we loop without a create, we'll create all the objects, and then fail to update the
      # 2nd time around cause we'll still be using a POST verb instead of a PATCH
      actionName: "create"
      namespaces: {{$namespaces}}
      tuningSet: RandomizedSaturationTimeLimited
      testMaxReplicaFactor: {{$RANDOM_SCALE_FACTOR}}
      daemonSetImage: k8s.gcr.io/pause:3.0
      daemonSetReplicas: 1
      bigDeploymentSize: {{$BIG_GROUP_SIZE}}
      bigDeploymentsPerNamespace: {{$bigDeploymentsPerNamespace}}
      mediumDeploymentSize: {{$MEDIUM_GROUP_SIZE}}
      mediumDeploymentsPerNamespace: {{$mediumDeploymentsPerNamespace}}
      smallDeploymentSize: {{$SMALL_GROUP_SIZE}}
      smallDeploymentsPerNamespace: {{$smallDeploymentsPerNamespace}}
      smallStatefulSetSize: {{$SMALL_GROUP_SIZE}}
      smallStatefulSetsPerNamespace: {{$SMALL_STATEFUL_SETS_PER_NAMESPACE}}
      mediumStatefulSetSize: {{$MEDIUM_GROUP_SIZE}}
      mediumStatefulSetsPerNamespace: {{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE}}
      bigJobSize: {{$BIG_GROUP_SIZE}}
      bigJobsPerNamespace: 1
      mediumJobSize: {{$MEDIUM_GROUP_SIZE}}
      mediumJobsPerNamespace: 1
      smallJobSize: {{$SMALL_GROUP_SIZE}}
      smallJobsPerNamespace: 1

- module:
    path: /modules/reconcile-objects.yaml
    params:
      actionName: "scale and update"
      namespaces: {{$namespaces}}
      tuningSet: forever-qps
      # tuningSet: RandomizedScalingTimeLimited
      randomScaleFactor: {{$RANDOM_SCALE_FACTOR}}
      testMaxReplicaFactor: {{$RANDOM_SCALE_FACTOR}}
      daemonSetImage: k8s.gcr.io/pause:3.1
      daemonSetReplicas: 1
      bigDeploymentSize: {{$BIG_GROUP_SIZE}}
      bigDeploymentsPerNamespace: {{$bigDeploymentsPerNamespace}}
      mediumDeploymentSize: {{$MEDIUM_GROUP_SIZE}}
      mediumDeploymentsPerNamespace: {{$mediumDeploymentsPerNamespace}}
      smallDeploymentSize: {{$SMALL_GROUP_SIZE}}
      smallDeploymentsPerNamespace: {{$smallDeploymentsPerNamespace}}
      smallStatefulSetSize: {{$SMALL_GROUP_SIZE}}
      smallStatefulSetsPerNamespace: {{$SMALL_STATEFUL_SETS_PER_NAMESPACE}}
      mediumStatefulSetSize: {{$MEDIUM_GROUP_SIZE}}
      mediumStatefulSetsPerNamespace: {{$MEDIUM_STATEFUL_SETS_PER_NAMESPACE}}
      bigJobSize: {{$BIG_GROUP_SIZE}}
      bigJobsPerNamespace: 1
      mediumJobSize: {{$MEDIUM_GROUP_SIZE}}
      mediumJobsPerNamespace: 1
      smallJobSize: {{$SMALL_GROUP_SIZE}}
      smallJobsPerNamespace: 1
      DISABLE_DAEMONSETS: true

# NOTE we removed job.yaml cause i couldn't figure out how to steady-state create/scale jobs
